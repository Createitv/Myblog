<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1"><meta name="robots" content="noodp"/><title>10分钟了解pytorch | 且慢</title><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content=""/>
<meta name="twitter:title" content="10分钟了解pytorch"/>
<meta name="twitter:description" content="Createitv"/><meta name="Description" content="Createitv"><meta property="og:title" content="10分钟了解pytorch" />
<meta property="og:description" content="Createitv" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://www.longtermbook.com/posts/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3pytorch/" /><meta property="og:image" content="http://www.longtermbook.com/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-13T17:53:01&#43;08:00" />
<meta property="article:modified_time" content="2021-10-13T17:53:01&#43;08:00" /><meta property="og:site_name" content="且慢" />

<meta name="application-name" content="Createitv">
<meta name="apple-mobile-web-app-title" content="Createitv"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
        <link rel="icon" type="image/png" sizes="192x192" href="/android-chrome-192x192.png">
        <link rel="icon" type="image/png" sizes="512x512" href="/android-chrome-512x512.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://www.longtermbook.com/posts/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3pytorch/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "10分钟了解pytorch",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/www.longtermbook.com\/posts\/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3pytorch\/"
        },"genre": "posts","keywords": "pytorch","wordCount":  1714 ,
        "url": "http:\/\/www.longtermbook.com\/posts\/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3pytorch\/","datePublished": "2021-10-13T17:53:01+08:00","dateModified": "2021-10-13T17:53:01+08:00","description": "Createitv"
    }
    </script><script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": [{
            "@type": "ListItem",
            "position": 1,
            "name": "主页",
            "item": "http:\/\/www.longtermbook.com\/"
        },{
            "@type": "ListItem",
            "position": 2,
            "name": "深度学习",
            "item": "http://www.longtermbook.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"
        },{
                "@type": "ListItem",
                "position": 3,
                "name": "10分钟了解pytorch"
            }]
    }
</script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="且慢" class="header-logo logo-svg">且慢</a>
        </div>
        <div class="menu">
            <nav>
                <ul class="menu-inner"><li>
                        <a class="menu-item" href="/posts/"> 文章 </a>
                    </li><li>
                        <a class="menu-item" href="/tags/"> 标签 </a>
                    </li><li>
                        <a class="menu-item" href="/categories/"> 分类 </a>
                    </li></ul>
            </nav><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <span class="svg-icon icon-search"></span>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <span class="svg-icon icon-cancel"></span>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <span class="svg-icon icon-loading"></span>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <span class="svg-icon icon-moon"></span>
                </a>
            </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="且慢" class="header-logo">且慢</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <span class="svg-icon icon-search"></span>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <span class="svg-icon icon-cancel"></span>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <span class="svg-icon icon-loading"></span>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><nav>
                <ul><li>
                        <a class="menu-item" href="/posts/" title="">文章</a>
                    </li><li>
                        <a class="menu-item" href="/tags/" title="">标签</a>
                    </li><li>
                        <a class="menu-item" href="/categories/" title="">分类</a>
                    </li></ul>
            </nav>
            <a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <span class="svg-icon icon-moon"></span>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div><main class="main">
<div class="container content-article  theme-classic"><div class="toc" id="toc-auto">
            <div class="toc-title">目录</div>
            <div class="toc-content" id="toc-content-auto"></div>
        </div>

    <div class="header-post">

        

        
        <div class="post-title">

                <div class="post-all-meta">
                    <nav class="breadcrumbs">
    <ol>
        <li><a href="/">主页 </a></li><li><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习 </a></li><li>10分钟了解pytorch</li>
    </ol>
</nav>
                    <h1 class="single-title flipInX">10分钟了解pytorch</h1><div class="post-meta">
                        <div class="post-meta-line"><span class="post-category">
                                <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><i class="svg-icon icon-folder"></i>深度学习</a>
                            </span><span class="post-meta-date">
                                <i class="svg-icon icon-clock"></i><time class="timeago" datetime="2021-10-13">2021-10-13</time>
                            </span><span class="post-meta-words">
                                <i class="svg-icon icon-pencil"></i>约 1714 字
                            </span>
                            <span class="post-meta-reading">
                                <i class="svg-icon icon-stopwatch"></i>预计阅读 4 分钟
                            </span>
                        </div>
                    </div>
                </div>

            </div>

            </div>

    <article class="single toc-start">

        <div class="content-block content-block-first content-block-position">

        <div class="post"><div class="details toc" id="toc-static"  data-kept="true">
                <div class="details-summary toc-title">
                    <span>目录</span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#tensor-的创建">Tensor 的创建</a></li>
    <li><a href="#tensor-的基本操作">tensor 的基本操作</a>
      <ul>
        <li><a href="#1-创建">1. 创建</a></li>
        <li><a href="#2-索引切片合并等转换">2. 索引、切片、合并等转换</a></li>
        <li><a href="#3-随机采样机器参数设置">3. 随机采样机器参数设置</a></li>
        <li><a href="#4-序列化与反序列化">4. 序列化与反序列化</a></li>
        <li><a href="#5-并行计算">5. 并行计算</a></li>
        <li><a href="#6-梯度控制">6. 梯度控制</a></li>
        <li><a href="#7-数学操作">7. 数学操作</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><h1 id="什么是-pytorch-">什么是 Pytorch ？</h1>
<ol>
<li>可以把 Pytorch 当做 Numpy 的替代方案使用，做科学计算，而且更强大。</li>
<li>做 AI 任务的模型建模、训练、部署等。这方面唯一能与其比肩的是 Google 家的 Tensorflow。</li>
</ol>
<p>本文是对 Pytorch 非常简短的一个介绍，目录如下：</p>
<ol>
<li>Tensor 的创建与操作，cuda 加速计算。</li>
<li>Pytorch 的自动微分功能。</li>
</ol>
<h1 id="tensor-创建与操作">Tensor 创建与操作</h1>
<h2 id="tensor-的创建">Tensor 的创建</h2>
<ol>
<li>创建一个未初始化的、形状为 (5,3) 的空 tensor。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">x = torch.empty(5, 3)
print(x)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[1.0102e-38, 9.0919e-39, 1.0102e-38],
        [8.9082e-39, 8.4489e-39, 9.6429e-39],
        [8.4490e-39, 9.6429e-39, 9.2755e-39],
        [1.0286e-38, 9.0919e-39, 8.9082e-39],
        [9.2755e-39, 8.4490e-39, 1.0194e-38]])
 
</code></pre></td></tr></table>
</div>
</div><ol>
<li>创建一个随机初始化，形状为 (5,3) 的 tensor。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">x = torch.rand(5, 3)
print(x)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[0.4133, 0.0885, 0.0503],
        [0.6771, 0.5543, 0.8236],
        [0.3047, 0.1217, 0.4441],
        [0.6269, 0.6820, 0.4217],
        [0.5631, 0.8517, 0.8708]])
 
</code></pre></td></tr></table>
</div>
</div><ol>
<li>创建一个0填充的、形状为 (5,3) 的 tensor。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">x = torch.zeros(5, 3, dtype=torch.long)
print(x)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
 
</code></pre></td></tr></table>
</div>
</div><p>直接从列表创建一个 tensor：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">x = torch.tensor([5.5, 3])
print(x)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tensor([5.5000, 3.0000])
</code></pre></td></tr></table>
</div>
</div><p>在现有 tensor 的基础上新建一个全1 tensor，默认情况下新的 tensor 会继承已有 tensor 的属性，比如形状、数据类型等，当然也可以手动指定。：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">x = x.new_ones(5, 3, dtype=torch.double)
print(x)
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
</code></pre></td></tr></table>
</div>
</div><p>生成一个与现有 tensor 一样形状的随机 tensor，且数据类型覆盖为 <code>torch.float</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">x = torch.randn_like(x, dtype=torch.float)
print(x)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[ 0.9629,  0.0349,  0.5597],
        [-2.1172,  1.1874, -0.1596],
        [ 0.6841, -0.6172, -0.4732],
        [ 0.0468, -0.3634,  1.1014],
        [ 0.6064,  0.1740,  0.2344]])
 
</code></pre></td></tr></table>
</div>
</div><ol>
<li>查看 tensor 的形状</li>
</ol>
<p>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">print(x.size())
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">torch.Size([5, 3])
 
</code></pre></td></tr></table>
</div>
</div><p><code>tensor.size</code>方法返回的是一个tuple对象，可以执行tuple的各种操作。</p>
<h2 id="tensor-的基本操作">tensor 的基本操作</h2>
<p>详细内容可参考<a href="https://link.juejin.cn/?target=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftorch.html%23indexing-slicing-joining-mutating-ops" target="_blank" rel="noopener noreffer">官方文档</a></p>
<h3 id="1-创建">1. 创建</h3>
<ul>
<li>torch.rand* 类随机生成 tensor 方法。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">torch.rand()
torch.rand_like()
torch.randn()
torch.randn_like()
torch.randint()
torch.randint_like()
torch.randperm()
torch.empty()
 
</code></pre></td></tr></table>
</div>
</div><ul>
<li>从其他数据源创建</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">torch.tensor()
torch.from_numpy()
torch.full()
torch.range()
torch.linspace()
torch.eye()
 
</code></pre></td></tr></table>
</div>
</div><h3 id="2-索引切片合并等转换">2. 索引、切片、合并等转换</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 按指定维度拼接 tensor
torch.cat() 

# 按指定维度将一个 tensor 分割为几个小块
torch.chunk()

# 按照深度方向（第三维）将多个 tensor 拼接，若tensor不足三维，则先转为3维 tensor 再行拼接。
torch.dstack()

# 将多个 tensor 按第1维进行拼接
torch.hstack()

# 按指定维度进行索引查询
torch.index_select()

# 根据一个 BoolTensor 进行 mask 查询
torch.masked_select()

# 调换 tensor 维度
torch.movedim()

# 返回 tensor 中非0值的索引
torch.nonzero()

# 改变 tensor 形状
torch.reshape()

# 按照指定维度及指定大小将 tensor 分割为几块
torch.split()

# 去除所有 tensor 中大小为1的维度，也可以指定哪个维度。
torch.squeeze()

# 按照指定维度拼接多个 tensor
torch.stack()

# tensor 转置
torch.t()

# 根据给定的索引从已有 tensor 中抽取出一个新的 tensor。
torch.take()

# 将 tensor 的指定两个维度进行互换
torch.transpose()

# 在指定地方增加 tensor 维度
torch.unsqueeze()

# 类似 dstack，只是是在第二个维度进行拼接
torch.vstack()

# 对 tensor 中每个元素根据条件判断如何返回
torch.where()
 
</code></pre></td></tr></table>
</div>
</div><h3 id="3-随机采样机器参数设置">3. 随机采样机器参数设置</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 设置非确定性随机数的随机种子
torch.seed

# 设置生成随机数的随机种子
torch.manual_seed

# 查看初始化的随机种子，为 `long` 类型
torch.initial_seed

# 查看随机数生成器状态
torch.get_rng_state

# 设置随机数生成器状态
torch.set_rng_state
 
</code></pre></td></tr></table>
</div>
</div><h3 id="4-序列化与反序列化">4. 序列化与反序列化</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 对 Pytorch 中的对象进行序列化保存和读取
torch.save()
torch.load()
 
</code></pre></td></tr></table>
</div>
</div><h3 id="5-并行计算">5. 并行计算</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># 获取和设置 CPU 并行操作时的线程数
torch.get_num_threads
torch.set_num_threads

# 获取和设置 CPU 上的互操作并行的线程数
torch.get_num_interop_threads
torch.set_num_interop_threads
 
</code></pre></td></tr></table>
</div>
</div><h3 id="6-梯度控制">6. 梯度控制</h3>
<p>有多重方法可以控制 tensor 是否计算梯度</p>
<p>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">x = torch.zeros(1, requires_grad=True)
with torch.no_grad():
    y = x * 2
print(y.requires_grad)


is_train = False
with torch.set_grad_enabled(is_train):
    y = x * 2
print(y.requires_grad)


torch.set_grad_enabled(True)
y = x * 2
print(y.requires_grad)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">False
False
True
 
</code></pre></td></tr></table>
</div>
</div><h3 id="7-数学操作">7. 数学操作</h3>
<p>pytorch 支持大部分常见的数学操作，这里不详细列举，详见<a href="https://link.juejin.cn/?target=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftorch.html%23indexing-slicing-joining-mutating-ops" target="_blank" rel="noopener noreffer">官方文档</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback"># Pointwise 操作
torch.abs
torch.clip
torch.cos
torch.sin
torch.div
torch.exp
torch.pow
torch.log
torch.sigmoid

# Reduction 操作
torch.argmax
torch.argmin
torch.max
torch.dist
torch.mean
torch.norm
torch.count_nonzero

# 比较操作
torch.allclose
torch.argsort
torch.eq
torch.equal
torch.ge
torch.gt
torch.isinf
torch.isfinite
torch.isnan
torch.isreal
torch.isneginf
torch.sort
torch.topk

# 光谱操作及其他操作
......
 
</code></pre></td></tr></table>
</div>
</div><h1 id="pytorch-的自动微分功能">Pytorch 的自动微分功能</h1>
<p>Pytorch 具备自动微分功能。</p>
<ol>
<li>requires_grad</li>
</ol>
<p>这个参数表示这个 tensor是否需要计算梯度。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">x = torch.ones(2, 2, requires_grad=True)
print(x)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
 
</code></pre></td></tr></table>
</div>
</div><ol>
<li>grad_fn</li>
</ol>
<p>指当前这个 tensor 是通过哪个函数得来的，在链式求导时会按照此函数进行计算。一般来讲除了用户自己创建的 tensor 外，如果是 pytorch 内置函数所生成的 tensor 都会有 grad_fn。</p>
<p>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">y = x + 2
print(y)
print(y.grad_fn)

z = y * y * 3
out = z.mean()
print(z, out)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
&lt;AddBackward0 object at 0x000001CEE32753C8&gt;
tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)
 
</code></pre></td></tr></table>
</div>
</div><ol>
<li>backward &amp; grad</li>
</ol>
<p>backward 方法用于计算梯度，通过链式求导法则计算好的梯度就存在 tensor 的 grad 属性中。</p>
<p>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">out.backward()
print(x.grad)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
 
</code></pre></td></tr></table>
</div>
</div><ol>
<li>with torch.no_grad</li>
</ol>
<p>通过这种方法，可以避免在计算中的梯度计算</p>
<p>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">print(x.requires_grad)
print((x ** 2).requires_grad)
with torch.no_grad():
    print((x ** 2).requires_grad)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">True
True
False
 
</code></pre></td></tr></table>
</div>
</div><ol>
<li>detach</li>
</ol>
<p>通过 detach 方法避免梯度计算</p>
<p>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">print(x.requires_grad)
y = x.detach()
print(y.requires_grad)
print(x.eq(y).all())
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">True
False
tensor(True)
 
</code></pre></td></tr></table>
</div>
</div><ol>
<li>requires_grad_</li>
</ol>
<p>设置 <code>requires_grad_</code> 属性可直接更改 tensor 梯度计算配置。</p>
<p>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)
 
</code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">False
True
&lt;SumBackward0 object at 0x000001CEE32753C8&gt;
 
</code></pre></td></tr></table>
</div>
</div></div>

            <div class="post" style="padding-top: 0">


<div class="post-share"><div class="share-link">
        <a class="share-icon share-facebook" href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="http://www.longtermbook.com/posts/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3pytorch/" data-hashtag="pytorch"><span class="svg-social-icon icon-facebook"></span></a>
    </div><div class="share-link">
        <a class="share-icon share-whatsapp" href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="http://www.longtermbook.com/posts/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3pytorch/" data-title="10分钟了解pytorch" data-web><span class="svg-social-icon icon-whatsapp"></span></a>
    </div><div class="share-link">
        <a class="share-icon share-line" href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="http://www.longtermbook.com/posts/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3pytorch/" data-title="10分钟了解pytorch"><span data-svg-src="/lib/simple-icons/icons/line.min.svg"></span></a>
</div><div class="share-link">
        <a class="share-icon share-evernote" href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="http://www.longtermbook.com/posts/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3pytorch/" data-title="10分钟了解pytorch"><span class="svg-social-icon icon-evernote"></span></a>
    </div></div>
<div class="post-tags"><a href="/tags/pytorch/" class="tag">pytorch</a></div></div>
        </div>
    <div id="toc-final"></div>
    </article></div>

</main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.82.0">Hugo</a> 强力驱动 | 主题 - <a href="https://ublogger.netlify.app/?utm_source=http://www.longtermbook.com/&utm_medium=footer&utm_campaign=config&utm_term=2.0.1" target="_blank" title="uBlogger 2.0.1">uBlogger</a>
                </div><div class="footer-line"><i class="svg-icon icon-copyright"></i><span>2018 - 2022</span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="svg-icon icon-arrow-up"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="svg-icon icon-comments-fixed"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><script src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script src="/lib/autocomplete/autocomplete.min.js"></script><script src="/lib/lunr/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="/lib/clipboard/clipboard.min.js"></script><script src="/lib/sharer/sharer.min.js"></script><script src="/lib/katex/katex.min.js"></script><script src="/lib/katex/auto-render.min.js"></script><script src="/lib/katex/copy-tex.min.js"></script><script src="/lib/katex/mhchem.min.js"></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":20},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":30,"type":"lunr"}};</script><script src="/js/theme.min.js"></script></body>
</html>
