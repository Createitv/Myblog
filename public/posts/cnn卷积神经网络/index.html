<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1"><meta name="robots" content="noodp"/><title>CNN卷积神经网络 | 且慢</title><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content=""/>
<meta name="twitter:title" content="CNN卷积神经网络"/>
<meta name="twitter:description" content="Createitv"/><meta name="Description" content="Createitv"><meta property="og:title" content="CNN卷积神经网络" />
<meta property="og:description" content="Createitv" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://www.longtermbook.com/posts/cnn%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" /><meta property="og:image" content="http://www.longtermbook.com/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-10-14T11:05:53&#43;08:00" />
<meta property="article:modified_time" content="2021-10-14T11:05:53&#43;08:00" /><meta property="og:site_name" content="且慢" />

<meta name="application-name" content="Createitv">
<meta name="apple-mobile-web-app-title" content="Createitv"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
        <link rel="icon" type="image/png" sizes="192x192" href="/android-chrome-192x192.png">
        <link rel="icon" type="image/png" sizes="512x512" href="/android-chrome-512x512.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://www.longtermbook.com/posts/cnn%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "CNN卷积神经网络",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/www.longtermbook.com\/posts\/cnn%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\/"
        },"genre": "posts","keywords": "CNN","wordCount":  4647 ,
        "url": "http:\/\/www.longtermbook.com\/posts\/cnn%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\/","datePublished": "2021-10-14T11:05:53+08:00","dateModified": "2021-10-14T11:05:53+08:00","description": "Createitv"
    }
    </script><script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": [{
            "@type": "ListItem",
            "position": 1,
            "name": "主页",
            "item": "http:\/\/www.longtermbook.com\/"
        },{
            "@type": "ListItem",
            "position": 2,
            "name": "深度学习",
            "item": "http://www.longtermbook.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"
        },{
                "@type": "ListItem",
                "position": 3,
                "name": "CNN卷积神经网络"
            }]
    }
</script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="且慢" class="header-logo logo-svg">且慢</a>
        </div>
        <div class="menu">
            <nav>
                <ul class="menu-inner"><li>
                        <a class="menu-item" href="/posts/"> 文章 </a>
                    </li><li>
                        <a class="menu-item" href="/tags/"> 标签 </a>
                    </li><li>
                        <a class="menu-item" href="/categories/"> 分类 </a>
                    </li></ul>
            </nav><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <span class="svg-icon icon-search"></span>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <span class="svg-icon icon-cancel"></span>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <span class="svg-icon icon-loading"></span>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <span class="svg-icon icon-moon"></span>
                </a>
            </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="且慢" class="header-logo">且慢</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <span class="svg-icon icon-search"></span>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <span class="svg-icon icon-cancel"></span>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <span class="svg-icon icon-loading"></span>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><nav>
                <ul><li>
                        <a class="menu-item" href="/posts/" title="">文章</a>
                    </li><li>
                        <a class="menu-item" href="/tags/" title="">标签</a>
                    </li><li>
                        <a class="menu-item" href="/categories/" title="">分类</a>
                    </li></ul>
            </nav>
            <a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <span class="svg-icon icon-moon"></span>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div><main class="main">
<div class="container content-article  theme-classic"><div class="toc" id="toc-auto">
            <div class="toc-title">目录</div>
            <div class="toc-content" id="toc-content-auto"></div>
        </div>

    <div class="header-post">

        

        
        <div class="post-title">

                <div class="post-all-meta">
                    <nav class="breadcrumbs">
    <ol>
        <li><a href="/">主页 </a></li><li><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习 </a></li><li>CNN卷积神经网络</li>
    </ol>
</nav>
                    <h1 class="single-title flipInX">CNN卷积神经网络</h1><div class="post-meta">
                        <div class="post-meta-line"><span class="post-category">
                                <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><i class="svg-icon icon-folder"></i>深度学习</a>
                            </span><span class="post-meta-date">
                                <i class="svg-icon icon-clock"></i><time class="timeago" datetime="2021-10-14">2021-10-14</time>
                            </span><span class="post-meta-words">
                                <i class="svg-icon icon-pencil"></i>约 4647 字
                            </span>
                            <span class="post-meta-reading">
                                <i class="svg-icon icon-stopwatch"></i>预计阅读 10 分钟
                            </span>
                        </div>
                    </div>
                </div>

            </div>

            </div>

    <article class="single toc-start">

        <div class="content-block content-block-first content-block-position">

        <div class="post"><div class="details toc" id="toc-static"  data-kept="true">
                <div class="details-summary toc-title">
                    <span>目录</span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-人是如何识别物体">1. 人是如何识别物体</a></li>
    <li><a href="#2-计算机是如何识别图片">2. 计算机是如何识别图片</a>
      <ul>
        <li><a href="#最简单的图像--二值图像">最简单的图像&ndash;二值图像</a></li>
        <li><a href="#二值图像在计算机中的表示">二值图像在计算机中的表示</a></li>
        <li><a href="#灰度图形在计算机中的表示">灰度图形在计算机中的表示</a></li>
        <li><a href="#彩色图像在计算机中的表示">彩色图像在计算机中的表示</a></li>
        <li><a href="#利用numpy读取图片矩阵">利用numpy读取图片矩阵</a></li>
      </ul>
    </li>
    <li><a href="#3-cnn卷积网络">3. CNN卷积网络</a>
      <ul>
        <li><a href="#发展史">发展史</a></li>
        <li><a href="#模型原理">模型原理</a></li>
        <li><a href="#输入层">输入层</a></li>
        <li><a href="#卷积层">卷积层</a></li>
        <li><a href="#池化层">池化层</a></li>
        <li><a href="#激活函数修正线性单元">激活函数(修正线性单元)</a></li>
        <li><a href="#全连接层">全连接层</a></li>
        <li><a href="#输出层">输出层</a></li>
        <li><a href="#卷积算力">卷积算力</a></li>
        <li><a href="#通用步骤">通用步骤</a></li>
        <li><a href="#超参数">超参数</a></li>
      </ul>
    </li>
    <li><a href="#4-mnist数据集pytorchcnn训练">4 MNIST数据集PytorchCNN训练</a></li>
    <li><a href="#参考">参考</a></li>
  </ul>
</nav></div>
            </div><p>本文以CNN算法的的领域之一图像识别为基础介绍CNN相关原理。</p>
<h2 id="1-人是如何识别物体">1. 人是如何识别物体</h2>
<p>人眼识别物体涉及脑神经，视觉神经。属于我的知识盲区，大致是从光照射到物体反射到眼球，第一层视网神经到各中电信号生物信号器官部位传导们最后统一到视觉皮层处理形成影像。</p>
<h2 id="2-计算机是如何识别图片">2. 计算机是如何识别图片</h2>
<p>图片的构成的基础单位像素，像素越大像素点越多。我们通常说的『图片分辨率』其实是指『像素数』（pixel count），通常表达为横向多少个像素x纵向多少个像素这样。像 480x800 这样的表述其实本来应该叫做尺寸（dimensions）的，但是因为数字图片并没有物理的长宽的概念，叫做尺寸反而可能会引起误解。数字图片的『宽』(width) 和『高』(height) 并非物理意义的长度单位，而是在两个维度上图片包含的像素个数。比如 480x800 这样的图片是由横向 480 个像素、纵向 800 个像素（合计 384 000 个像素点）构成的。




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014121417356.png"
         alt="image-20211014121417356"
         title="image-20211014121417356.png"
    /></p>
<h3 id="最简单的图像--二值图像">最简单的图像&ndash;二值图像</h3>
<p>只有黑白。黑色代表前景色，白色代表背景色。Pytorch中MNIST手写数据集就是二值图像通道数为1。</p>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014121643414.png"
         alt="image-20211014121643414"
         title="image-20211014121643414.png"
    /></p>
<h3 id="二值图像在计算机中的表示">二值图像在计算机中的表示</h3>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014121836063.png"
         alt="image-20211014121836063"
         title="image-20211014121836063.png"
    /></p>
<h3 id="灰度图形在计算机中的表示">灰度图形在计算机中的表示</h3>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014122224467.png"
         alt="image-20211014122224467"
         title="image-20211014122224467.png"
    /></p>
<h3 id="彩色图像在计算机中的表示">彩色图像在计算机中的表示</h3>
<p>颜色是我们对到达视网膜的各种频率的光的感觉。我们的视网膜有三种颜色感光视锥细胞，负责接收不同频率的光。这些感光器分类分别对应于红、绿和蓝三种颜色。人眼可以觉察的其他颜色都能由这三种颜色混合而成。这也是日常中最常见的颜色图。RGB三原色共同组成了任意色值像素。</p>
<p>以下为一些RGB像素值组成的颜色，也就是一个像素代表三个值（R， G， B）</p>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014122855563.png"
         alt="image-20211014122855563"
         title="image-20211014122855563.png"
    /></p>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014123026284.png"
         alt="image-20211014123026284"
         title="image-20211014123026284.png"
    /></p>
<p>彩色图像在pytorch也就是所谓的三通道Chanel就等于3。</p>
<h3 id="利用numpy读取图片矩阵">利用numpy读取图片矩阵</h3>
<p>图片样例：</p>
<p>




<img loading="lazy" decoding="async"
         src="https://img2.baidu.com/it/u=1928696135,3556146331&amp;fm=26&amp;fmt=auto"
         alt="img"
         title="u=1928696135,3556146331&amp;fm=26&amp;fmt=auto"
    /></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="k">def</span> <span class="nf">read_img</span><span class="p">(</span><span class="n">img_name</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_name</span><span class="p">)</span>
    <span class="c1"># 将读取的图像变为numpy矩阵</span>
    <span class="n">np_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>  <span class="c1"># (224, 224, 3)</span>
    <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    将3维变为4维矩阵，可以用reshape去做变换，
</span><span class="s2">    因为只有一张图片，可以加一个数组，然后让它封装在一个列表中，
</span><span class="s2">    它就会把列表的这一维也加上去，列表的这一维就是1.
</span><span class="s2">    &#34;&#34;&#34;</span>
    <span class="n">np_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">np_img</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>  <span class="c1"># (1, 224, 224, 3)</span>
    <span class="k">return</span> <span class="n">np_img</span>

<span class="n">im</span> <span class="o">=</span> <span class="n">read_img</span><span class="p">(</span><span class="s1">&#39;1.jpg&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">im</span><span class="p">))</span>
<span class="c1"># &lt;class &#39;numpy.ndarray&#39;&gt;</span>

<span class="k">print</span><span class="p">(</span><span class="n">im</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="c1"># uint8</span>
<span class="k">print</span><span class="p">(</span><span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># (1, 499, 500, 3)</span>
<span class="k">print</span><span class="p">(</span><span class="n">im</span><span class="p">)</span> <span class="c1"># 得到图片在计算机中表示的三维数组</span>

<span class="o">-----------</span><span class="err">代表一个三维数组，中间省略大部分</span><span class="o">-----------</span>
<span class="p">[[[[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
   <span class="o">...</span>
   <span class="p">[</span><span class="mi">129</span> <span class="mi">169</span>  <span class="mi">39</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">168</span> <span class="mi">201</span>  <span class="mi">99</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">230</span> <span class="mi">255</span> <span class="mi">170</span><span class="p">]]</span>
  <span class="p">[[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
   <span class="o">...</span>
   <span class="p">[</span><span class="mi">129</span> <span class="mi">169</span>  <span class="mi">39</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">168</span> <span class="mi">201</span>  <span class="mi">99</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">230</span> <span class="mi">255</span> <span class="mi">170</span><span class="p">]]</span>
  <span class="p">[[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
   <span class="o">...</span>
   <span class="p">[</span><span class="mi">129</span> <span class="mi">169</span>  <span class="mi">39</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">168</span> <span class="mi">201</span>  <span class="mi">99</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">230</span> <span class="mi">255</span> <span class="mi">170</span><span class="p">]]</span>
  <span class="o">...</span>
  <span class="p">[[</span>  <span class="mi">3</span> <span class="mi">166</span> <span class="mi">239</span><span class="p">]</span>
   <span class="p">[</span>  <span class="mi">3</span> <span class="mi">166</span> <span class="mi">239</span><span class="p">]</span>
   <span class="p">[</span>  <span class="mi">3</span> <span class="mi">166</span> <span class="mi">239</span><span class="p">]</span>
   <span class="o">...</span>
   <span class="p">[</span><span class="mi">231</span> <span class="mi">189</span>  <span class="mi">55</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">242</span> <span class="mi">211</span>  <span class="mi">98</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">255</span> <span class="mi">238</span> <span class="mi">135</span><span class="p">]]</span>
  <span class="p">[[</span>  <span class="mi">3</span> <span class="mi">166</span> <span class="mi">239</span><span class="p">]</span>
   <span class="p">[</span>  <span class="mi">3</span> <span class="mi">166</span> <span class="mi">239</span><span class="p">]</span>
   <span class="p">[</span>  <span class="mi">3</span> <span class="mi">166</span> <span class="mi">239</span><span class="p">]</span>
   <span class="o">...</span>
   <span class="p">[</span><span class="mi">231</span> <span class="mi">189</span>  <span class="mi">55</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">242</span> <span class="mi">211</span>  <span class="mi">98</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">255</span> <span class="mi">238</span> <span class="mi">135</span><span class="p">]]</span>
  <span class="p">[[</span>  <span class="mi">3</span> <span class="mi">166</span> <span class="mi">239</span><span class="p">]</span>
   <span class="p">[</span>  <span class="mi">3</span> <span class="mi">166</span> <span class="mi">239</span><span class="p">]</span>
   <span class="p">[</span>  <span class="mi">3</span> <span class="mi">166</span> <span class="mi">239</span><span class="p">]</span>
   <span class="o">...</span>
   <span class="p">[</span><span class="mi">231</span> <span class="mi">189</span>  <span class="mi">55</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">242</span> <span class="mi">211</span>  <span class="mi">98</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">255</span> <span class="mi">238</span> <span class="mi">135</span><span class="p">]]]]</span>

<span class="c1"># 输出第一排前5个像素点</span>
<span class="k">print</span><span class="p">(</span><span class="n">im</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">])</span>
<span class="p">[[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]</span>
 <span class="p">[</span><span class="mi">245</span>  <span class="mi">82</span>  <span class="mi">37</span><span class="p">]]</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="3-cnn卷积网络">3. CNN卷积网络</h2>
<p>CNN：Convolutional Neural Network（卷积神经网络）</p>
<p>




<img loading="lazy" decoding="async"
         src="https://pic1.zhimg.com/80/v2-1614367fe2f6a8a0fe64c06b8e437e0b_1440w.jpg?source=1940ef5c"
         alt="img"
         title="v2-1614367fe2f6a8a0fe64c06b8e437e0b_1440w.jpg?source=1940ef5c"
    /></p>
<h3 id="发展史">发展史</h3>
<p>卷积神经网络 (Convolutional Neural Network, CNN) 是一种目前广泛用于图像，自然语言处理等领域的深度神经网络模型。1998 年，Lecun 等人 <a href="https://leovan.me/cn/2018/08/cnn/#fn:1" target="_blank" rel="noopener noreffer">1</a> 提出了一种基于梯度的反向传播算法用于文档的识别。在这个神经网络中，卷积层 (Convolutional Layer) 扮演着至关重要的角色。</p>
<p>随着运算能力的不断增强，一些大型的 CNN 网络开始在图像领域中展现出巨大的优势，2012 年，Krizhevsky 等人 <a href="https://leovan.me/cn/2018/08/cnn/#fn:2" target="_blank" rel="noopener noreffer">2</a> 提出了 AlexNet 网络结构，并在 ImageNet 图像分类竞赛 <a href="https://leovan.me/cn/2018/08/cnn/#fn:3" target="_blank" rel="noopener noreffer">3</a> 中以超过之前 11% 的优势取得了冠军。随后不同的学者提出了一系列的网络结构并不断刷新 ImageNet 的成绩，其中比较经典的网络包括：VGG (Visual Geometry Group) <a href="https://leovan.me/cn/2018/08/cnn/#fn:4" target="_blank" rel="noopener noreffer">4</a>，GoogLeNet <a href="https://leovan.me/cn/2018/08/cnn/#fn:5" target="_blank" rel="noopener noreffer">5</a> 和 ResNet <a href="https://leovan.me/cn/2018/08/cnn/#fn:6" target="_blank" rel="noopener noreffer">6</a>。</p>
<p>CNN 在图像分类问题上取得了不凡的成绩，同时一些学者也尝试将其应用在图像的其他领域，例如：物体检测 <a href="https://leovan.me/cn/2018/08/cnn/#fn:7" target="_blank" rel="noopener noreffer">7</a><a href="https://leovan.me/cn/2018/08/cnn/#fn:8" target="_blank" rel="noopener noreffer">8</a><a href="https://leovan.me/cn/2018/08/cnn/#fn:9" target="_blank" rel="noopener noreffer">9</a>，语义分割 <a href="https://leovan.me/cn/2018/08/cnn/#fn:10" target="_blank" rel="noopener noreffer">10</a>，图像摘要 <a href="https://leovan.me/cn/2018/08/cnn/#fn:11" target="_blank" rel="noopener noreffer">11</a>，行为识别 <a href="https://leovan.me/cn/2018/08/cnn/#fn:12" target="_blank" rel="noopener noreffer">12</a> 等。除此之外，在非图像领域 CNN 也取得了一定的成绩 <a href="https://leovan.me/cn/2018/08/cnn/#fn:13" target="_blank" rel="noopener noreffer">13</a>。</p>
<h3 id="模型原理">模型原理</h3>
<p>下图为 Lecun 等人提出的 LeNet-5 的网络架构：</p>
<p>




<img loading="lazy" decoding="async"
         src="https://leovan.me/images/cn/2018-08-25-cnn/lenet-5.png"
         alt="LeNet-5"
         title="lenet-5.png"
    /></p>
<p>下面我们针对 CNN 网络中的不同类型的网络层逐一进行介绍。</p>
<h3 id="输入层">输入层</h3>
<p>LeNet-5 解决的手写数字分类问题的输入为一张 32x32 像素的灰度图像 (Gray Scale)。日常生活中计算机常用的图像的表示方式为 RGB，即将一张图片分为红色通道 (Red Channel)，绿色通道 (Green Channel) 和蓝色通道 (Blue Channel)，其中每个通道的每个像素点的数值范围为 [0,255]。灰度图像表示该图片仅包含一个通道，也就是不具备彩色信息，每个像素点的数值范围同 RGB 图像的取值范围相同。</p>
<p>因此，一张图片在计算机的眼里就是一个如下图所示的数字矩阵 (示例图片来自于 MNIST 数据集：</p>
<p>




<img loading="lazy" decoding="async"
         src="https://leovan.me/images/cn/2018-08-25-cnn/digit-pixels.png"
         alt="Digit-Pixels"
         title="digit-pixels.png"
    /></p>
<p>在将图像输入到 CNN 网络之前，通常我们会对其进行预处理，因为每个像素点的最大取值为 255，因此将每个像素点的值除以 255 则可以将其归一化到 [0,1] 的范围。</p>
<h3 id="卷积层">卷积层</h3>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014153731931.png"
         alt="image-20211014153731931"
         title="image-20211014153731931.png"
    /></p>
<p>第一次卷积计算之后的结果</p>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014153924363.png"
         alt="image-20211014153924363"
         title="image-20211014153924363.png"
    /></p>
<p>在了解卷积层之前，让我们先来了解一下什么是卷积？</p>
<p>下图形象的刻画了利用一个 3x3 大小的卷积核的整个卷积计算过程：</p>
<p>




<img loading="lazy" decoding="async"
         src="https://leovan.me/images/cn/2018-08-25-cnn/conv-sobel.gif"
         alt="Conv-Sobel"
         title="conv-sobel.gif"
    /></p>
<p>一些预设的卷积核对于图片可以起到不同的滤波器效果，例如下面 4 个卷积核分别会对图像产生不同的效果：不改变，边缘检测，锐化和高斯模糊。</p>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014152509181.png"
         alt="image-20211014152509181"
         title="image-20211014152509181.png"
    /></p>
<p>对 lena 图片应用这 4 个卷积核，变换后的效果如下 (从左到右，从上到下)：</p>
<p>




<img loading="lazy" decoding="async"
         src="https://leovan.me/images/cn/2018-08-25-cnn/lena-filters.png"
         alt="Lena-Filters"
         title="lena-filters.png"
    /></p>
<p>在上面整个计算卷积的动图中，我们不难发现，利用 3x3 大小 (我们一般将这个参数称之为 <code>kernel_size</code>，即<strong>卷积核的大小</strong>，其可以为一个整数表示长宽大小相同，也可以为两个不同的整数) 的卷积核对 5x5 大小的原始矩阵进行卷积操作后，结果矩阵并没有保持原来的大小，而是变为了 (5-(3-1))x(5-(3-1)) (即 3x3) 大小的矩阵。这就需要引入 CNN 网络中卷积层的两个常用参数 <code>padding</code> 和 <code>strides</code>。</p>
<p><code>padding</code> 是指是否对图像的外侧进行<strong>补零操作</strong>，其取值一般为 <code>VALID</code> 和 <code>SAME</code> 两种。<code>VALID</code> 表示<strong>不进行补零</strong>操作，对于输入形状为 (x,y) 的矩阵，利用形状为 (m,n) 的卷积核进行卷积，得到的结果矩阵的形状则为 (x−m+1,y−n+1)。</p>
<p>




<img loading="lazy" decoding="async"
         src="https://leovan.me/images/cn/2018-08-25-cnn/conv-zero-padding.png"
         alt="Conv2d-Zero-Padding"
         title="conv-zero-padding.png"
    /></p>
<p><code>strides</code> 是指进行卷积操作时，每次卷积核移动的步长。示例中，卷积核在横轴和纵轴方向上的移动步长均为 1，除此之外用于也可以指定不同的步长。移动的步长同样会对卷积后的结果的形状产生影响。</p>
<p>除此之外，还有另一个重要的参数 <code>filters</code>，其表示在一个卷积层中使用的<strong>卷积核的个数</strong>。在一个卷积层中，一个卷积核可以学习并提取图像的一种特征，但往往图片中包含多种不同的特征信息，因此我们需要多个不同的卷积核提取不同的特征。下图 <a href="https://leovan.me/cn/2018/08/cnn/#fn:15" target="_blank" rel="noopener noreffer">15</a> 是一个利用 4 个不同的卷积核对一张图像进行卷积操作的示意图：</p>
<p>




<img loading="lazy" decoding="async"
         src="https://leovan.me/images/cn/2018-08-25-cnn/conv2d-kernels.png"
         alt="Conv2d-Kernels"
         title="conv2d-kernels.png"
    /></p>
<p>上面我们都是以一个灰度图像 (仅包含 1 个通道) 为示例进行的讨论，那么对于一个 RGB 图像 (包含 3 个通道)，相应的，卷积核也是一个 3 维的形状，如所示：</p>
<p>




<img loading="lazy" decoding="async"
         src="https://leovan.me/images/cn/2018-08-25-cnn/conv3d-kernels.png"
         alt="Conv3d-Kernels"
         title="conv3d-kernels.png"
    /></p>
<h3 id="池化层">池化层</h3>
<p><strong>池化层</strong> 是一个利用 <strong>池化函数 (pooling function)</strong> 对网络输出进行进一步调整的网络层。池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。常用的池化函数包括最大池化 (max pooling) 函数 (即给出邻域内的最大值) 和平均池化 (average pooling) 函数 (即给出邻域内的平均值) 等。但无论选择何种池化函数，当对输入做出少量平移时，池化对输入的表示都近似 <strong>不变 (invariant)</strong>。<strong>局部平移不变性</strong> 是一个很重要的性质，尤其是当我们关心某个特征是否出现而不关心它出现的位置时。</p>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014154418398.png"
         alt="image-20211014154418398"
         title="image-20211014154418398.png"
    /></p>
<p>池化层同卷积层类似，具有三个比较重要的参数：<code>pool_size</code>，<code>strides</code> 和 <code>padding</code>，分别表示池化窗口的大小，步长以及是否对图像的外侧进行补零操作。下图 是一个 <code>pool_size=3</code>，<code>strides=3</code>，<code>padding='valid'</code> 的最大池化过程示例：</p>
<p>




<img loading="lazy" decoding="async"
         src="https://leovan.me/images/cn/2018-08-25-cnn/max-pooling.gif"
         alt="Max-Pooling"
         title="max-pooling.gif"
    /></p>
<p>池化层同时也能够提高网络的计算效率，例如上图中在横轴和纵轴的步长均为 3，经过池化后，下一层网络节点的个数降低至前一层的 13×3=19。</p>
<h3 id="激活函数修正线性单元">激活函数(修正线性单元)</h3>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014154859516.png"
         alt="image-20211014154859516"
         title="image-20211014154859516.png"
    /></p>
<h3 id="全连接层">全连接层</h3>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014155812562.png"
         alt="image-20211014155812562"
         title="image-20211014155812562.png"
    /></p>
<p>全链接层 (Fully-connected or Dense Layer) 的目的就是将我们最后一个池化层的输出连接到最终的输出节点上。例如，最后一个池化层的输出大小为 [5×5×16]，也就是有 5×5×16=400 个节点，对于手写数字识别的问题，我们的输出为 0 至 9 共 10 个数字，采用 one-hot 编码的话，输出层共 10 个节点。例如在 LeNet 中有 2 个全连接层，每层的节点数分别为 120 和 84，在实际应用中，通常全连接层的节点数会逐层递减。需要注意的是，在进行编码的时候，第一个全连接层并不是直接与最后一个池化层相连，而是先对池化层进行 flatten 操作，使其变成一个一维向量后再与全连接层相连。</p>
<h3 id="输出层">输出层</h3>
<p>输出层根据具体问题的不同会略有不同，例如对于手写数字识别问题，采用 one-hot 编码的话，输出层则包含 10 个节点。对于回归或二分类问题，输出层则仅包含 1 个节点。当然对于二分类问题，我们也可以像多分类问题一样将其利用 one-hot 进行编码，例如 [1,0] 表示类型 0，[0,1] 表示类型 1。</p>
<h3 id="卷积算力">卷积算力</h3>
<p>一长800x600的彩色图片共1440000个像素点，好三个3x3的卷机核取计算大概需要1300万次乘法和1200次加法。</p>
<h3 id="通用步骤">通用步骤</h3>
<p>卷积 -&raquo; 非线性激活 -&raquo; 池化 == 降维</p>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014155134790.png"
         alt="image-20211014155134790"
         title="image-20211014155134790.png"
    /></p>
<p>多层神经网络</p>
<p>重复卷积、非线性激活、池化，运算量也不断增大</p>
<p>




<img loading="lazy" decoding="async"
         src="https://typora-1300715298.cos.ap-shanghai.myqcloud.com/uPic/image-20211014155245645.png"
         alt="image-20211014155245645"
         title="image-20211014155245645.png"
    /></p>
<h3 id="超参数">超参数</h3>
<p>训练之前定义的参数：卷积核尺寸，卷积核数目。池化步长，全连接数量。</p>
<h2 id="4-mnist数据集pytorchcnn训练">4 MNIST数据集PytorchCNN训练</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># mnist 训练集</span>
<span class="c1"># 1.导入训练包</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="c1"># 2. 定义超参数</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># 每次批处理的数据</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>  <span class="c1"># 是否使用GPU</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># 训练轮次</span>

<span class="c1"># 3 构建pipleline，对图形做处理</span>
<span class="n">pipleline</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>  <span class="c1"># 图片转化维张量</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>  <span class="c1"># 正则化，过拟合时候降低复制度</span>
<span class="p">])</span>

<span class="c1"># 4 下载加载数据</span>
<span class="c1"># 下载数据集</span>
<span class="n">train_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s2">&#34;./data&#34;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">pipleline</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s2">&#34;./data&#34;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">pipleline</span><span class="p">)</span>

<span class="c1"># 加载数据集</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>


<span class="c1"># 5 构建神经网络</span>
<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># 1图片通道，10：输出通道， 5 卷积核大小</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># 10输入通道，20：输出通道， 3 卷积核大小</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fce1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>  <span class="c1"># 20 * 10 * 10 输入通道， 500：输出通道</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fce2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># 500 输入通道， 10：输出通道</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="s2">&#34;&#34;&#34;前向传播&#34;&#34;&#34;</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 输入：batch*1*28*28 输出batch*10*24*24 24等于图片像素-卷积核大小+1</span>
        <span class="c1"># 非线性激活</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># shape不变 输出 batch*10*24*24</span>
        <span class="c1"># 池化</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 2步长，2池化核心大小 输入batch*10*24*24 输出batch*10*12*12</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 输入batch*10*12*12  输出batch*20*10*10  10等于图片像素-卷积核大小+1</span>
        <span class="c1"># 非线性激活</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># 拉伸</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># -1 自动计算维度 20*10*10 = 2000</span>
        <span class="c1"># 全连接</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fce1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 输入：batch*2000  输出batch*500</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fce2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 输入：batch*500 输出batch*10</span>
        <span class="c1"># 损失函数</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 计算分类后， 每个数字的概率值, 得到损失函数</span>
        <span class="k">return</span> <span class="n">output</span>


<span class="c1"># 6 定义优化器</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>  <span class="c1"># Adam 优化器</span>


<span class="c1"># 7 定义训练方法</span>
<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="c1"># 模型训练</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch_index</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="c1"># 部署到devices</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># 初始化梯度</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># 训练后的结果</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="c1"># 交叉墒， 计算损失</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>  <span class="c1"># 真是值和目标值</span>
        <span class="c1"># 找到概率值最大的下标</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># 反向传播</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># 参数优化</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">batch_index</span> <span class="o">%</span> <span class="mi">3000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&#34;Train Epoch : {} </span><span class="se">\t</span><span class="s2"> Loss : {:.6f}loss&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>

<span class="c1"># 8 定义测试方法</span>
<span class="k">def</span> <span class="nf">test_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">):</span>
    <span class="c1"># 模型验证</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="c1"># 正确率</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="c1"># 测试损失</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  <span class="c1"># 不会计算梯度和反向传播</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
            <span class="c1"># 部署到DEVICE</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># 测试数据</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="c1"># 计算测试损失</span>
            <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="c1"># 找到概率值最大的下标</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># 累计正确的值</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="c1"># 平均损失值</span>
        <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span>
            <span class="s2">&#34;Test Average loss : {:.4f} Accuracy : {:.3f}&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">,</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)))</span>


<span class="c1"># 9 定义调用方法</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPOCHS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="n">test_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span><span class="n">test_loader</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=AFlIM0jSI9I" target="_blank" rel="noopener noreffer">人脸识别啥原理？人工智能（二）卷积神经网络</a></li>
<li><a href="https://www.zhihu.com/zvideo/1402038708572524544?playTime=641.3" target="_blank" rel="noopener noreffer">计算机视觉——图像的表示</a></li>
<li><a href="https://leovan.me/cn/2018/08/cnn/" target="_blank" rel="noopener noreffer">卷积神经网络 (Convolutional Neural Network, CNN)</a></li>
<li><a href="https://www.youtube.com/watch?v=FmpDIaiMIeA" target="_blank" rel="noopener noreffer">How Convolutional Neural Networks work</a></li>
</ul>
</div>

            <div class="post" style="padding-top: 0">


<div class="post-share"><div class="share-link">
        <a class="share-icon share-facebook" href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="http://www.longtermbook.com/posts/cnn%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-hashtag="CNN"><span class="svg-social-icon icon-facebook"></span></a>
    </div><div class="share-link">
        <a class="share-icon share-whatsapp" href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="http://www.longtermbook.com/posts/cnn%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-title="CNN卷积神经网络" data-web><span class="svg-social-icon icon-whatsapp"></span></a>
    </div><div class="share-link">
        <a class="share-icon share-line" href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="http://www.longtermbook.com/posts/cnn%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-title="CNN卷积神经网络"><span data-svg-src="/lib/simple-icons/icons/line.min.svg"></span></a>
</div><div class="share-link">
        <a class="share-icon share-evernote" href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="http://www.longtermbook.com/posts/cnn%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-title="CNN卷积神经网络"><span class="svg-social-icon icon-evernote"></span></a>
    </div></div>
<div class="post-tags"><a href="/tags/cnn/" class="tag">CNN</a></div></div>
        </div>
    <div id="toc-final"></div>
    </article></div>

</main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.82.0">Hugo</a> 强力驱动 | 主题 - <a href="https://ublogger.netlify.app/?utm_source=http://www.longtermbook.com/&utm_medium=footer&utm_campaign=config&utm_term=2.0.1" target="_blank" title="uBlogger 2.0.1">uBlogger</a>
                </div><div class="footer-line"><i class="svg-icon icon-copyright"></i><span>2018 - 2021</span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="svg-icon icon-arrow-up"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="svg-icon icon-comments-fixed"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><script src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script src="/lib/autocomplete/autocomplete.min.js"></script><script src="/lib/lunr/lunr.min.js"></script><script src="/lib/lunr/lunr.stemmer.support.min.js"></script><script src="/lib/lunr/lunr.zh.min.js"></script><script src="/lib/clipboard/clipboard.min.js"></script><script src="/lib/sharer/sharer.min.js"></script><script src="/lib/katex/katex.min.js"></script><script src="/lib/katex/auto-render.min.js"></script><script src="/lib/katex/copy-tex.min.js"></script><script src="/lib/katex/mhchem.min.js"></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":20},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":30,"type":"lunr"}};</script><script src="/js/theme.min.js"></script></body>
</html>
